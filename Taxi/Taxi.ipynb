{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc930e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfa22c",
   "metadata": {},
   "source": [
    "## Create the training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c5e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiQLearning:\n",
    "    learning_rate = 0.9\n",
    "    discount_factor = 0.9\n",
    "    epsilon = 1\n",
    "\n",
    "    def train(self, episodes):\n",
    "        # initialize the environment\n",
    "        env = gym.make('Taxi-v3')\n",
    "        # initialize the q-table\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))  # 500 possible states, 6 possible actions => 500 x 6\n",
    "        # init arrays used for plotting\n",
    "        mean_rewards = []\n",
    "        epsilon_history = []\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                if random.random() < self.epsilon:\n",
    "                    # pick a random action\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # pick the most optimal action\n",
    "                    action = np.argmax(q[state, :])\n",
    "\n",
    "                # take a step\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # new values in the q-table based on reward that we got\n",
    "                q[state, action] = q[state, action] + self.learning_rate * (reward + self.discount_factor * np.max(q[new_state, :]) - q[state, action])\n",
    "\n",
    "                # move to the next state\n",
    "                state = new_state\n",
    "\n",
    "            # decay epsilon\n",
    "            self.epsilon = self.epsilon - 1 / episodes\n",
    "            self.epsilon = max(self.epsilon, 0)\n",
    "\n",
    "            # this is used for plotting\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if i % 50 == 0 and i > 0:\n",
    "                epsilon_history.append(self.epsilon)\n",
    "                mean_rewards.append(np.mean(rewards_per_episode[-49]))\n",
    "\n",
    "            if self.epsilon == 0:\n",
    "                self.learning_rate = 0.0001\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        self.save_q_table(q)\n",
    "\n",
    "        self.plot_results(mean_rewards, epsilon_history)\n",
    "\n",
    "    def test(self, episodes):\n",
    "        env = gym.make('Taxi-v3', render_mode='human')\n",
    "\n",
    "        q = self.load_q_table()\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # take only the most optimal actions\n",
    "                action = np.argmax(q[state, :])\n",
    "                state, reward, done, _ = env.step(action)\n",
    "        env.close()\n",
    "\n",
    "    def plot_results(self, rewards, epsilon_history):\n",
    "        plt.clf()\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Mean Rewards\")\n",
    "        plt.plot(rewards, color=\"orange\", label=\"learning rate\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"resources\\\\learning_curve.png\")\n",
    "\n",
    "        plt.clf()\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Epsilon Value\")\n",
    "        plt.plot(epsilon_history, color=\"orange\", label=\"epsilon decay\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"resources\\\\epsilon_decay.png\")\n",
    "\n",
    "    def save_q_table(self, q_table):\n",
    "        f = open(\"resources\\\\taxi.pkl\", \"wb\")\n",
    "        pickle.dump(q_table, f)\n",
    "        f.close()\n",
    "\n",
    "    def load_q_table(self):\n",
    "        f = open(\"resources\\\\taxi.pkl\", \"rb\")\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414db1fc",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f51246",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lau\\miniconda3\\envs\\rlProjects3.7\\lib\\site-packages\\gym\\core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "C:\\Users\\Lau\\miniconda3\\envs\\rlProjects3.7\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
     ]
    }
   ],
   "source": [
    "taxi = TaxiQLearning()\n",
    "# taxi.train(10000)\n",
    "taxi.test(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
