{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a717d6",
   "metadata": {},
   "source": [
    "## Creating the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)\n",
    "        self.out = nn.Linear(h1_nodes, out_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615895c0",
   "metadata": {},
   "source": [
    "## Creating the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe94279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686b05a",
   "metadata": {},
   "source": [
    "## Creating the Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d14f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarDQL:\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.01  # learning rate (alpha)\n",
    "    discount_factor_g = 0.9  # discount rate (gamma)\n",
    "    network_sync_rate = 50000  # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 100000  # size of replay memory\n",
    "    mini_batch_size = 32  # size of the training data set sampled from the replay memory\n",
    "    num_divisions = 20\n",
    "    position_space = None\n",
    "    velocity_space = None\n",
    "\n",
    "    loss_fn = nn.MSELoss()  # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None  # NN Optimizer. Initialize later.\n",
    "\n",
    "    def train(self, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.position_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)  # Between -1.2 and 0.6\n",
    "        self.velocity_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)  # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1\n",
    "        memory = ReplayBuffer(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()  # Initialize to state 0\n",
    "            done = False\n",
    "            rewards = 0\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"In the {i}th episode we have high score: {best_rewards}\")\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while not done and rewards > -1000:\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample()  # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, done))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count += 1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(rewards)\n",
    "            epsilon_history.append(epsilon)\n",
    "            if done:\n",
    "                goal_reached = True\n",
    "\n",
    "            if rewards > best_rewards:\n",
    "                best_rewards = rewards\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), \"resources\\\\mountain_car_dql.pt\")\n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory) > self.mini_batch_size and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1 / episodes, 0)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count = 0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.plot(rewards_per_episode, color=\"orange\", label=\"learning rate\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"resources\\\\learning_curve.png\")\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.clf()\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Epsilon Value\")\n",
    "        plt.plot(epsilon_history, color=\"orange\", label=\"epsilon decay\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"resources\\\\epsilon_decay.png\")\n",
    "\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                # Agent receive reward of 0 for reaching goal.\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        state_p = np.digitize(state[0], self.position_space)\n",
    "        state_v = np.digitize(state[1], self.velocity_space)\n",
    "\n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "\n",
    "    # Run the FrozeLake environment with the learned policy\n",
    "    def test(self, episodes):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.position_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)  # Between -1.2 and 0.6\n",
    "        self.velocity_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)  # Between -0.07 and 0.07\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        policy_dqn.load_state_dict(torch.load(\"resources\\\\mountain_car_dql.pt\"))\n",
    "        policy_dqn.eval()  # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while not done:\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state, reward, done, _ = env.step(action)\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700524d0",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car = MountainCarDQL()\n",
    "mountain_car.train(50000)\n",
    "# mountain_car.test(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
